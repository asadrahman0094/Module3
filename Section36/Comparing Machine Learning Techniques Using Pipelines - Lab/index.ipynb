{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Techniques Using Pipelines - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'lll use a Dataset created by Otto group, which was also used in a [Kaggle competition](https://www.kaggle.com/c/otto-group-product-classification-challenge/data).\n",
    "\n",
    "The description of the data set is as follows:\n",
    "\n",
    "The Otto Group is one of the worldâ€™s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). They are selling millions of products worldwide every day, with several thousand products being added to our product line.\n",
    "\n",
    "A consistent analysis of the performance of our products is crucial. However, due to their global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights Otto Group can generate about their product range.\n",
    "\n",
    "In this lab, you'll use a data set containing:\n",
    "- A column `id`, which is an anonymous id unique to a product\n",
    "- 93 columns `feat_1`, `feat_2`, ..., `feat_93`, which are the various features of a product\n",
    "- a column `target` - the class of a product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "- Compare different classification techniques\n",
    "- Construct pipelines in scikit-learn\n",
    "- Use pipelines in combination with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be following the data science workflow:\n",
    "\n",
    "1. Initial data inspection, exploratory data analysis, and cleaning\n",
    "2. Feature engineering and selection\n",
    "3. create a baseline model\n",
    "4. create a machine learning pipeline and compare results with the baseline model\n",
    "5. Interpret the model and draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial data inspection, exploratory data analysis, and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in \"otto_group.csv\".\n",
    "\n",
    "Things to do here:\n",
    "- Check for NAs\n",
    "- Check the distributions\n",
    "- Check how many inputs there are\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"otto_group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only looked at some of the features here, make sure to do a quick check for all the features (similar structure)\n",
    "feat = data.loc[:, 'feat_1':'feat_30']\n",
    "feat.hist(figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at all the histograms, you can tell that a lot of the data are zero-inflated, so most of the variables contain mostly zeros and then some higher values here and there. No normality, but for most machine learning techniques this is not an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.boxplot(figsize=(10,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is zero-inflated the boxplots look as shown above. Because there are this many zeroes, most values above zero will seem to be outliers. The safe decision for this data is to not delete any outliers and see what happens. With many 0s, sparse data is available and high values may be super informative. More-over, without having any intuitive meaning for each of the features, we don't know if a value of ~260 is actually an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there any missing data?\n",
    "\n",
    "feat.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and selection with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the correlation structure of your features using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = data.loc[:, 'feat_1':'feat_93']\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(feat.corr(), center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA to downscale your features. Use PCA to select a number of features in a way that you still keep 80% of your explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1 = PCA(n_components=20)\n",
    "pca_2 = PCA(n_components=40)\n",
    "pca_3 = PCA(n_components=60)\n",
    "\n",
    "principalComponents = pca_1.fit_transform(feat)\n",
    "principalComponents = pca_2.fit_transform(feat)\n",
    "principalComponents = pca_3.fit_transform(feat)\n",
    "\n",
    "print(np.sum(pca_1.explained_variance_ratio_))\n",
    "print(np.sum(pca_2.explained_variance_ratio_))\n",
    "print(np.sum(pca_3.explained_variance_ratio_))\n",
    "\n",
    "pca = PCA(n_components=27)\n",
    "principalComponents = pca.fit_transform(feat)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(pd.DataFrame(principalComponents).corr(), center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train test split with a test size of 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively big training set. Feel free to make it smaller (down to ~20%), but for an initial run you can try smaller training sets so the computation time is more manageable.\n",
    "\n",
    "For now, simply use the original data and not the principal components. We looked at the PC's first to get a sense of our correlation structure, and to see how we can downsize our data without losing too much information. In what's next, you'll make PCA part of the pipeline!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['target']\n",
    "X = data.loc[:, 'feat_1':'feat_93']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your baseline model *in a pipeline setting*. In the pipeline\n",
    "- Your first step will be to scale your features down to the number of features that ensure you keep just 80% of your explained variance (which we saw before)\n",
    "- Your second step will be the building a basic logistic regression model.\n",
    "\n",
    "Make sure to fit the model using the training set, and test the result by obtaining the accuracy using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some pipelines\n",
    "pipe_lr = Pipeline([('pca', PCA(n_components=27, random_state=123)),\n",
    "         ('clf', LogisticRegression(random_state=123))])\n",
    "\n",
    "# Fit the pipelines\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "print(pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pipe_lr.predict(X_test) == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline consisting of a linear SVM, a simple Decision Tree and a simple Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above, but now create three different pipelines:\n",
    "- One for a standard linear SCM\n",
    "- One for a default decision tree\n",
    "- One for a RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "## KEEP IT FOR NOW\n",
    "# Construct some pipelines\n",
    "pipe_svm = Pipeline([('pca', PCA(n_components=27)),\n",
    "        ('clf', svm.SVC(random_state=123))])\n",
    "        \n",
    "pipe_tree = Pipeline([('pca', PCA(n_components=27)),\n",
    "        ('clf', tree.DecisionTreeClassifier(random_state=123))])\n",
    "\n",
    "pipe_rf = Pipeline([('pca', PCA(n_components=27)),\n",
    "        ('clf', RandomForestClassifier(random_state=123))])\n",
    "\n",
    "# List of pipelines, List of pipeline names\n",
    "pipelines = [pipe_svm, pipe_tree, pipe_rf]\n",
    "pipeline_names = ['Support Vector Machine','Decision Tree','Random Forest']\n",
    "\n",
    "# Loop to fit each of the three pipelines\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "# Compare accuracies\n",
    "for index, val in enumerate(pipelines):\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct 3 pipelines with grid search\n",
    "- one for support vector machines - make sure your grid isn't too big. You'll see it takes quite a while to fit SVMs with non-linear kernel functions!\n",
    "- one for random forests - try to have around 40 different models\n",
    "- one for the adaboost algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM pipeline with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "pipe_svm = Pipeline([('pca', PCA(n_components=27)),\n",
    "            ('clf', svm.SVC(random_state=123))])\n",
    "\n",
    "# Set grid search params\n",
    "param_grid_svm = [\n",
    "  {'clf__C': [0.1, 1, 10]  , 'clf__kernel': ['linear']},\n",
    "  {'clf__C': [1, 10], 'clf__gamma': [0.001, 0.01], 'clf__kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# Construct grid search\n",
    "gs_svm = GridSearchCV(estimator=pipe_svm,\n",
    "            param_grid=param_grid_svm,\n",
    "            scoring='accuracy',\n",
    "            cv=3, verbose=2, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_svm.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your grid search object along with `.cv_results` to get the full result overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svm.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest pipeline with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "pipe_rf = Pipeline([('pca', PCA(n_components=27)),\n",
    "            ('clf', RandomForestClassifier(random_state = 123))])\n",
    "\n",
    "# Set grid search params\n",
    "param_grid_forest = [ \n",
    "  {'clf__n_estimators': [120],\n",
    "   'clf__criterion': ['entropy', 'gini'], \n",
    "   'clf__max_depth': [4, 5, 6],  \n",
    "   'clf__min_samples_leaf':[0.05 ,0.1, 0.2],  \n",
    "   'clf__min_samples_split':[0.05 ,0.1, 0.2]\n",
    "  }\n",
    "]\n",
    "\n",
    "# Construct grid search\n",
    "gs_rf = GridSearchCV(estimator=pipe_rf,\n",
    "            param_grid=param_grid_forest,\n",
    "            scoring='accuracy',\n",
    "            cv=3, verbose=2, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_rf.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Construct pipeline\n",
    "pipe_ab = Pipeline([('pca', PCA(n_components=27)),\n",
    "            ('clf', AdaBoostClassifier(random_state = 123))])\n",
    "\n",
    "# Set grid search params\n",
    "adaboost_param_grid = {\n",
    "    'clf__n_estimators': [30, 50, 70],\n",
    "    'clf__learning_rate': [1.0, 0.5, 0.1]\n",
    "}\n",
    "\n",
    "# Construct grid search\n",
    "gs_ab = GridSearchCV(estimator=pipe_ab,\n",
    "            param_grid=adaboost_param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=3, verbose=2, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_ab.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_ab.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_ab.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "Note that this solution is only one of many options. The results in the Random Forest and Adaboost models show that there is a lot of improvement possible tuning the hyperparameters further, so make sure to explore this yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Great! You now got a lot of practice in. What algorithm would you choose and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
